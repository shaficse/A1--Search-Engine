{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec (Skipgram )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/shafisourov/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n",
      "taiwan had a trade trade surplus of how much billion dlrs last year .\n",
      "1 the paper gave no further details . 4.435082897543907\n",
      "2 no decisions are likely until after indonesia ' s elections on april 23 , traders said . 2.5840189307928085\n",
      "3 much more serious for hong kong is the disadvantage of action restraining trade , '' he said . 1.8180981278419495\n",
      "4 nainggolan said that the exchange was trying to boost overseas interest by building up contacts with end - users . 1.775369793176651\n",
      "5 but other businessmen said such a short - term commercial advantage would be outweighed by further u . s . pressure to block imports . 1.6946449875831604\n",
      "6 `` we are aware of the seriousness of the u . s . 1.6228081658482552\n",
      "7 the analysts agreed the bank was aggressive . 1.6163814812898636\n",
      "8 miti is planning to work out a revised energy supply / demand outlook through deliberations of committee meetings of the agency of natural resources and energy , the officials said . 1.510283537209034\n",
      "9 miti is expected to lower the projection for primary energy supplies in the year 2000 to 550 mln kilolitres ( kl ) from 600 mln , they said . 1.475901436060667\n",
      "10 `` that is a very short - term view , '' said lawrence mills , director - general of the federation of hong kong industry . 1.4746800996363163\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "import json \n",
    "import string\n",
    "\n",
    "# Ensure you have downloaded the necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "class Skipgram(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(Skipgram, self).__init__()\n",
    "        self.embedding_center  = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
    "    \n",
    "    def forward(self, center, outside, all_vocabs):\n",
    "        center_embedding     = self.embedding_center(center)  #(batch_size, 1, emb_size)\n",
    "        outside_embedding    = self.embedding_center(outside) #(batch_size, 1, emb_size)\n",
    "        all_vocabs_embedding = self.embedding_center(all_vocabs) #(batch_size, voc_size, emb_size)\n",
    "        \n",
    "        top_term = torch.exp(outside_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2))\n",
    "        #batch_size, 1, emb_size) @ (batch_size, emb_size, 1) = (batch_size, 1, 1) = (batch_size, 1) \n",
    "\n",
    "        lower_term = all_vocabs_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2)\n",
    "        #batch_size, voc_size, emb_size) @ (batch_size, emb_size, 1) = (batch_size, voc_size, 1) = (batch_size, voc_size) \n",
    "        \n",
    "        lower_term_sum = torch.sum(torch.exp(lower_term), 1)  #(batch_size, 1)\n",
    "        \n",
    "        loss = -torch.mean(torch.log(top_term / lower_term_sum))  #scalar\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "word2index_path = './config_model_files/word2index.json'  \n",
    "index2word_path = './config_model_files/index2word.json' \n",
    "model_path = './config_model_files/word2vec_model.pth'\n",
    "config_path = './config_model_files/word2vec_config.json'\n",
    "corpus_path =  './config_model_files/corpus.txt'\n",
    "with open(word2index_path, 'r') as file:\n",
    "    word2index = json.load(file)  # Load the word2index dictionary from the JSON file\n",
    "\n",
    "with open(index2word_path, 'r') as file:\n",
    "    index2word = json.load(file)  # Load the index2word dictionary from the JSON file\n",
    "    \n",
    "# Load the model's configuration from a JSON file\n",
    "with open(config_path, 'r') as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "# Retrieve the configuration values\n",
    "voc_size = config['voc_size']  # Vocabulary size\n",
    "emb_size = config['emb_size']  # Embedding size\n",
    "\n",
    "# Initialize a new Word2Vec model with the loaded configuration\n",
    "loaded_model = Skipgram(voc_size, emb_size)\n",
    "\n",
    "# Load the state dictionary (model parameters) into the initialized model\n",
    "loaded_model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# Set the model to evaluation mode (useful for inference)\n",
    "loaded_model.eval()\n",
    "\n",
    "# Confirm successful model loading\n",
    "print(\"Model loaded successfully\")\n",
    "\n",
    "\n",
    "# Define the file path to your corpus text file\n",
    "file_path = corpus_path  # Replace with your actual file path\n",
    "\n",
    "def load_corpus(file_path):\n",
    "    corpus = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Strip removes leading/trailing whitespace\n",
    "            line = line.strip()\n",
    "            # print(line)\n",
    "            if line:  # Add non-empty lines to the corpus\n",
    "                corpus.append(line)\n",
    "    return corpus\n",
    "\n",
    "# Load the corpus\n",
    "corpus = load_corpus(file_path)\n",
    "\n",
    "def preprocess(text):\n",
    "    # Tokenizes the text into words and converts all characters to lowercase\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    return tokens\n",
    "\n",
    "def get_embedding(text, model, word2index):\n",
    "    \"\"\"\n",
    "    Converts a text input to its corresponding average embedding.\n",
    "    \"\"\"\n",
    "    tokens = preprocess(text)  # Preprocess the text to get tokens\n",
    "    embeddings = []\n",
    "    for token in tokens:\n",
    "        index = word2index.get(token, word2index.get('<UNK>'))\n",
    "        word_tensor = torch.LongTensor([index])\n",
    "\n",
    "        embed_center = model.embedding_center(word_tensor)\n",
    "        embed_outside = model.embedding_outside(word_tensor)\n",
    "        embed = (embed_center + embed_outside) / 2\n",
    "        embeddings.append(embed.detach().numpy())\n",
    "    \n",
    "    # Average the embeddings\n",
    "    if embeddings:\n",
    "        embeddings = np.array(embeddings)\n",
    "        text_embedding = np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        text_embedding = np.zeros(model.embedding_center.weight.shape[1])\n",
    "    \n",
    "    # Make sure the embedding is a 1-D array\n",
    "    text_embedding = text_embedding.flatten()  # Flatten the array to ensure it's 1-D\n",
    "    \n",
    "    return text_embedding\n",
    "\n",
    "def retrieve_top_passages(query, corpus, model, word2index, top_n=10):\n",
    "    \"\"\"\n",
    "    Computes the dot product between the input query and each passage in the corpus,\n",
    "    and retrieves the top N most similar passages.\n",
    "    \"\"\"\n",
    "    query_embedding = get_embedding(query, model, word2index)\n",
    "    similarities = []\n",
    "\n",
    "    for passage in corpus:\n",
    "        passage_embedding = get_embedding(passage, model, word2index)\n",
    "        similarity = np.dot(query_embedding, passage_embedding)\n",
    "        similarities.append(similarity)\n",
    "\n",
    "    \n",
    "    sorted_indices = sorted(range(len(similarities)), key=lambda i: similarities[i], reverse=True)\n",
    "    top_indices = sorted_indices[:top_n]\n",
    "    \n",
    "    # Normalize the scores to be percentages of the max score\n",
    "    # max_score = max([similarities[idx] for idx in top_indices])\n",
    "    # top_passages = [(corpus[idx], (similarities[idx] / max_score) * 100) for idx in top_indices]\n",
    " \n",
    "    top_passages = [(corpus[idx], (similarities[idx]) * 100) for idx in top_indices]\n",
    "    \n",
    "    return top_passages\n",
    "\n",
    "# ... [Load corpus and run example usage here as in your code] ...\n",
    "\n",
    "# Example usage with the retrieve_top_passages function\n",
    "query = \"taiwan had a trade trade surplus of how much billion dlrs last year .\".lower()\n",
    "print(query)\n",
    "top_passages = retrieve_top_passages(query, corpus, loaded_model, word2index, top_n=10)\n",
    "i = 1\n",
    "for passage, score in top_passages:\n",
    "    print(i, passage, score)\n",
    "    i+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/shafisourov/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n",
      "taiwan had a trade trade surplus of how much billion dlrs last year .\n",
      "1 officials say the infant exchange has made a good start although trading in coffee has been disappointing . 5.8266885578632355\n",
      "2 the fledgling exchange currently trades coffee and rubber physicals on an open outcry system four days a week . 5.520188808441162\n",
      "3 physical rubber trading was launched in 1985 , with coffee added in january 1986 . 5.410360544919968\n",
      "4 annual ore capacity will be about 750 , 000 tonnes . 4.828231409192085\n",
      "5 the trade ministry and exchange board are considering the introduction of futures trading later for rubber , but one official said a feasibility study was needed first . 4.7112710773944855\n",
      "6 he said teams had already been to south korea and taiwan to encourage direct use of the exchange , while a delegation would also visit europe , mexico and some latin american states to encourage participation . 4.592230170965195\n",
      "7 trading in either crude palm oil ( cpo ) or refined palm oil may also be introduced . 4.4519975781440735\n",
      "8 they said the shipment was for april 8 to 20 delivery . 3.9516333490610123\n",
      "9 robusta coffee grades four and five are traded for prompt delivery and up to five months forward , exchange officials said . 3.3450454473495483\n",
      "10 the u . s . has said it will impose 300 mln dlrs of tariffs on imports of japanese electronics goods on april 17 , in retaliation for japan ' s alleged failure to stick to a pact not to sell semiconductors on world markets at below cost . 3.0281253159046173\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "import json \n",
    "import string\n",
    "\n",
    "# Ensure you have downloaded the necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "class SkipgramNeg(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(SkipgramNeg, self).__init__()\n",
    "        self.embedding_center  = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
    "        self.logsigmoid        = nn.LogSigmoid()\n",
    "    \n",
    "    def forward(self, center, outside, negative):\n",
    "        #center, outside:  (bs, 1)\n",
    "        #negative       :  (bs, k)\n",
    "        \n",
    "        center_embed   = self.embedding_center(center) #(bs, 1, emb_size)\n",
    "        outside_embed  = self.embedding_outside(outside) #(bs, 1, emb_size)\n",
    "        negative_embed = self.embedding_outside(negative) #(bs, k, emb_size)\n",
    "        \n",
    "        uovc           = outside_embed.bmm(center_embed.transpose(1, 2)).squeeze(2) #(bs, 1)\n",
    "        ukvc           = -negative_embed.bmm(center_embed.transpose(1, 2)).squeeze(2) #(bs, k)\n",
    "        ukvc_sum       = torch.sum(ukvc, 1).reshape(-1, 1) #(bs, 1)\n",
    "        \n",
    "        loss           = self.logsigmoid(uovc) + self.logsigmoid(ukvc_sum)\n",
    "        \n",
    "        return -torch.mean(loss)\n",
    "        \n",
    "word2index_path = './config_model_files/word2index_neg_sam.json'  \n",
    "index2word_path = './config_model_files/index2word_neg_sam.json' \n",
    "model_path = './config_model_files/word2vec_model_neg_sam.pth'\n",
    "config_path = './config_model_files/word2vec_config_neg_sam.json'\n",
    "corpus_path =  './config_model_files/corpus_neg_sam.txt'\n",
    "with open(word2index_path, 'r') as file:\n",
    "    word2index = json.load(file)  # Load the word2index dictionary from the JSON file\n",
    "\n",
    "with open(index2word_path, 'r') as file:\n",
    "    index2word = json.load(file)  # Load the index2word dictionary from the JSON file\n",
    "    \n",
    "# Load the model's configuration from a JSON file\n",
    "with open(config_path, 'r') as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "# Retrieve the configuration values\n",
    "voc_size = config['voc_size']  # Vocabulary size\n",
    "emb_size = config['emb_size']  # Embedding size\n",
    "\n",
    "# Initialize a new Word2Vec model with the loaded configuration\n",
    "loaded_model = SkipgramNeg(voc_size, emb_size)\n",
    "\n",
    "# Load the state dictionary (model parameters) into the initialized model\n",
    "loaded_model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# Set the model to evaluation mode (useful for inference)\n",
    "loaded_model.eval()\n",
    "\n",
    "# Confirm successful model loading\n",
    "print(\"Model loaded successfully\")\n",
    "\n",
    "\n",
    "# Define the file path to your corpus text file\n",
    "file_path = corpus_path  # Replace with your actual file path\n",
    "\n",
    "def load_corpus(file_path):\n",
    "    corpus = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Strip removes leading/trailing whitespace\n",
    "            line = line.strip()\n",
    "            # print(line)\n",
    "            if line:  # Add non-empty lines to the corpus\n",
    "                corpus.append(line)\n",
    "    return corpus\n",
    "\n",
    "# Load the corpus\n",
    "corpus = load_corpus(file_path)\n",
    "\n",
    "def preprocess(text):\n",
    "    # Tokenizes the text into words and converts all characters to lowercase\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    return tokens\n",
    "\n",
    "def get_embedding(text, model, word2index):\n",
    "    \"\"\"\n",
    "    Converts a text input to its corresponding average embedding.\n",
    "    \"\"\"\n",
    "    tokens = preprocess(text)  # Preprocess the text to get tokens\n",
    "    embeddings = []\n",
    "    for token in tokens:\n",
    "        index = word2index.get(token, word2index.get('<UNK>'))\n",
    "        word_tensor = torch.LongTensor([index])\n",
    "\n",
    "        embed_center = model.embedding_center(word_tensor)\n",
    "        embed_outside = model.embedding_outside(word_tensor)\n",
    "        embed = (embed_center + embed_outside) / 2\n",
    "        embeddings.append(embed.detach().numpy())\n",
    "    \n",
    "    # Average the embeddings\n",
    "    if embeddings:\n",
    "        embeddings = np.array(embeddings)\n",
    "        text_embedding = np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        text_embedding = np.zeros(model.embedding_center.weight.shape[1])\n",
    "    \n",
    "    # Make sure the embedding is a 1-D array\n",
    "    text_embedding = text_embedding.flatten()  # Flatten the array to ensure it's 1-D\n",
    "    \n",
    "    return text_embedding\n",
    "\n",
    "def retrieve_top_passages(query, corpus, model, word2index, top_n=10):\n",
    "    \"\"\"\n",
    "    Computes the dot product between the input query and each passage in the corpus,\n",
    "    and retrieves the top N most similar passages.\n",
    "    \"\"\"\n",
    "    query_embedding = get_embedding(query, model, word2index)\n",
    "    similarities = []\n",
    "\n",
    "    for passage in corpus:\n",
    "        passage_embedding = get_embedding(passage, model, word2index)\n",
    "        similarity = np.dot(query_embedding, passage_embedding)\n",
    "        similarities.append(similarity)\n",
    "\n",
    "    \n",
    "    sorted_indices = sorted(range(len(similarities)), key=lambda i: similarities[i], reverse=True)\n",
    "    top_indices = sorted_indices[:top_n]\n",
    "    \n",
    "    # Normalize the scores to be percentages of the max score\n",
    "    # max_score = max([similarities[idx] for idx in top_indices])\n",
    "    # top_passages = [(corpus[idx], (similarities[idx] / max_score) * 100) for idx in top_indices]\n",
    " \n",
    "    top_passages = [(corpus[idx], (similarities[idx]) * 100) for idx in top_indices]\n",
    "    \n",
    "    return top_passages\n",
    "\n",
    "# ... [Load corpus and run example usage here as in your code] ...\n",
    "\n",
    "# Example usage with the retrieve_top_passages function\n",
    "query = \"taiwan had a trade trade surplus of how much billion dlrs last year .\".lower()\n",
    "print(query)\n",
    "top_passages = retrieve_top_passages(query, corpus, loaded_model, word2index, top_n=10)\n",
    "i = 1\n",
    "for passage, score in top_passages:\n",
    "    print(i, passage, score)\n",
    "    i+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/shafisourov/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n",
      "what other businessmen said for commercial advantage\n",
      "1 miti is expected to lower the projection for primary energy supplies in the year 2000 to 550 mln kilolitres ( kl ) from 600 mln , they said . 12.003625929355621\n",
      "2 but other businessmen said such a short - term commercial advantage would be outweighed by further u . s . pressure to block imports . 7.376070320606232\n",
      "3 the fledgling exchange currently trades coffee and rubber physicals on an open outcry system four days a week . 7.233703136444092\n",
      "4 the department said first quarter exports expanded to 60 . 6 billion baht from 56 . 6 billion . 7.018963992595673\n",
      "5 `` we are aware of the seriousness of the u . s . 6.892098486423492\n",
      "6 they said they could not say how long the disruption will go on and what effect it will have on shipping movements . 6.390486657619476\n",
      "7 taiwan had a trade trade surplus of 15 . 6 billion dlrs last year , 95 pct of it with the u . s . 6.325379014015198\n",
      "8 the country ' s oil import bill , however , fell 23 pct in the first quarter due to lower oil prices . 6.223846226930618\n",
      "9 the paper gave no further details . 6.141806021332741\n",
      "10 threat against japan because it serves as a warning to us , '' said a senior taiwanese trade official who asked not to be named . 6.1353858560323715\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "import json \n",
    "import string\n",
    "\n",
    "# Ensure you have downloaded the necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "class Glove(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(Glove, self).__init__()\n",
    "        # Embeddings for center words\n",
    "        self.center_embedding  = nn.Embedding(voc_size, emb_size)\n",
    "        # Embeddings for context (outside) words\n",
    "        self.outside_embedding = nn.Embedding(voc_size, emb_size)\n",
    "        \n",
    "        # Bias terms for center words\n",
    "        self.center_bias       = nn.Embedding(voc_size, 1) \n",
    "        # Bias terms for context (outside) words\n",
    "        self.outside_bias      = nn.Embedding(voc_size, 1)\n",
    "    \n",
    "    def forward(self, center, outside, coocs, weighting):\n",
    "        # Retrieve the embeddings for the center words\n",
    "        center_embeds  = self.center_embedding(center)  # (batch_size, 1, emb_size)\n",
    "        # Retrieve the embeddings for the outside words\n",
    "        outside_embeds = self.outside_embedding(outside)  # (batch_size, 1, emb_size)\n",
    "        \n",
    "        # Retrieve and squeeze the bias for the center words\n",
    "        center_bias    = self.center_bias(center).squeeze(1)\n",
    "        # Retrieve and squeeze the bias for the outside words\n",
    "        target_bias    = self.outside_bias(outside).squeeze(1)\n",
    "        \n",
    "        # Compute the dot product of center and outside word embeddings\n",
    "        inner_product  = outside_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        \n",
    "        # Compute the GloVe loss as the weighted squared error between\n",
    "        # the log co-occurrence counts and the model predictions (dot product + biases)\n",
    "        loss = weighting * torch.pow(inner_product + center_bias + target_bias - coocs, 2)\n",
    "        \n",
    "        # Return the sum of the losses for the batch\n",
    "        return torch.sum(loss)\n",
    "\n",
    "        \n",
    "word2index_path = './config_model_files/word2index_Glove_Scratch.json'  \n",
    "index2word_path = './config_model_files/index2word_Glove_Scratch.json' \n",
    "model_path = './config_model_files/word2vec_model_Glove_Scratch.pth'\n",
    "config_path = './config_model_files/word2vec_config_Glove_Scratch.json'\n",
    "corpus_path =  './config_model_files/corpus_Glove_Scratch.txt'\n",
    "with open(word2index_path, 'r') as file:\n",
    "    word2index = json.load(file)  # Load the word2index dictionary from the JSON file\n",
    "\n",
    "with open(index2word_path, 'r') as file:\n",
    "    index2word = json.load(file)  # Load the index2word dictionary from the JSON file\n",
    "    \n",
    "# Load the model's configuration from a JSON file\n",
    "with open(config_path, 'r') as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "# Retrieve the configuration values\n",
    "voc_size = config['voc_size']  # Vocabulary size\n",
    "emb_size = config['emb_size']  # Embedding size\n",
    "\n",
    "# Initialize a new Word2Vec model with the loaded configuration\n",
    "loaded_model = Glove(voc_size, emb_size)\n",
    "\n",
    "# Load the state dictionary (model parameters) into the initialized model\n",
    "loaded_model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# Set the model to evaluation mode (useful for inference)\n",
    "loaded_model.eval()\n",
    "\n",
    "# Confirm successful model loading\n",
    "print(\"Model loaded successfully\")\n",
    "\n",
    "\n",
    "# Define the file path to your corpus text file\n",
    "file_path = corpus_path  # Replace with your actual file path\n",
    "\n",
    "def load_corpus(file_path):\n",
    "    corpus = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Strip removes leading/trailing whitespace\n",
    "            line = line.strip()\n",
    "            # print(line)\n",
    "            if line:  # Add non-empty lines to the corpus\n",
    "                corpus.append(line)\n",
    "    return corpus\n",
    "\n",
    "# Load the corpus\n",
    "corpus = load_corpus(file_path)\n",
    "\n",
    "def preprocess(text):\n",
    "    # Tokenizes the text into words and converts all characters to lowercase\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    return tokens\n",
    "\n",
    "def get_embedding(text, model, word2index):\n",
    "    \"\"\"\n",
    "    Converts a text input to its corresponding average embedding.\n",
    "    \"\"\"\n",
    "    tokens = preprocess(text)  # Preprocess the text to get tokens\n",
    "    embeddings = []\n",
    "    for token in tokens:\n",
    "        index = word2index.get(token, word2index.get('<UNK>'))\n",
    "        word_tensor = torch.LongTensor([index])\n",
    "\n",
    "        embed_center = model.center_embedding(word_tensor)\n",
    "        embed_outside = model.outside_embedding(word_tensor)\n",
    "        embed = (embed_center + embed_outside) / 2\n",
    "        embeddings.append(embed.detach().numpy())\n",
    "    \n",
    "    # Average the embeddings\n",
    "    if embeddings:\n",
    "        embeddings = np.array(embeddings)\n",
    "        text_embedding = np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        text_embedding = np.zeros(model.center_embedding.weight.shape[1])\n",
    "    \n",
    "    # Make sure the embedding is a 1-D array\n",
    "    text_embedding = text_embedding.flatten()  # Flatten the array to ensure it's 1-D\n",
    "    \n",
    "    return text_embedding\n",
    "\n",
    "def retrieve_top_passages(query, corpus, model, word2index, top_n=10):\n",
    "    \"\"\"\n",
    "    Computes the dot product between the input query and each passage in the corpus,\n",
    "    and retrieves the top N most similar passages.\n",
    "    \"\"\"\n",
    "    query_embedding = get_embedding(query, model, word2index)\n",
    "    similarities = []\n",
    "\n",
    "    for passage in corpus:\n",
    "        passage_embedding = get_embedding(passage, model, word2index)\n",
    "        similarity = np.dot(query_embedding, passage_embedding)\n",
    "        similarities.append(similarity)\n",
    "\n",
    "    \n",
    "    sorted_indices = sorted(range(len(similarities)), key=lambda i: similarities[i], reverse=True)\n",
    "    top_indices = sorted_indices[:top_n]\n",
    "    \n",
    "    top_passages = [(corpus[idx], (similarities[idx]) * 100) for idx in top_indices]\n",
    "    \n",
    "    return top_passages\n",
    "\n",
    "# ... [Load corpus and run example usage here as in your code] ...\n",
    "\n",
    "# Example usage with the retrieve_top_passages function\n",
    "# query = \"taiwan had a trade surplus of how much billion dlrs last year\"\n",
    "query = \"taiwan had a trade surplus of how much billion dlrs last year\"\n",
    "query = 'what other businessmen said for commercial advantage'\n",
    "print(query)\n",
    "top_passages = retrieve_top_passages(query, corpus, loaded_model, word2index, top_n=10)\n",
    "i = 1\n",
    "for passage, score in top_passages:\n",
    "    print(i, passage, score)\n",
    "    i+= 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f2c79af21be9d001248940c049b6176cf8bfb45cabf7aa85848f5cea0f590f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
