{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports essential for neural network modeling and NLP include NumPy for numerical operations, PyTorch for model construction, and Gensim for managing word embeddings, crucial for models like GloVe and applying pre-trained embeddings in NLP tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # Fundamental package for scientific computing with Python\n",
    "import torch  # PyTorch, a deep learning framework\n",
    "import torch.nn as nn  # Submodule of PyTorch specifically for neural network layers\n",
    "from torch.nn import functional as F  # Functional interface containing typical operations used for building neural networks\n",
    "import json  # Module for JSON file parsing\n",
    "\n",
    "# Gensim related imports for handling word vectors\n",
    "from gensim.test.utils import datapath  # Utility function to handle file paths within Gensim\n",
    "from gensim.models import KeyedVectors  # Class to handle word vectors in Gensim\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec  # Script to convert GloVe format to Word2Vec format\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Compare  training loss & training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we present a comparative analysis of three prominent word embedding models: Skip-gram, Skip-gram with Negative Sampling, and GloVe. The comparison is based on two critical metrics: training loss and training time.\n",
    "<span style=\"color:magenta;\">*Data for this analysis is sourced from the individual notebooks dedicated to each model.*</span>\n",
    "\n",
    "\n",
    "\n",
    "The models were trained on a selected subset of the Reuters corpus from NLTK, containing 500 passages out of a total of 54,716, and 2677 tokens out of 1720917. The training performance was assessed based on the average training loss and the total time taken for training\n",
    "\n",
    "\n",
    "#### Training Loss\n",
    "| Model                          | Average Training Loss |\n",
    "|--------------------------------|-----------------------|\n",
    "| Skip-gram                      | 8.133966            |\n",
    "| Skip-gram with Negative Sampling | 1.977957             |\n",
    "| GloVe Scratch                        | 0.724803 |\n",
    "\n",
    "- **Training Loss:**\n",
    "- Skip-gram: Exhibited an average training loss of 8.133966, indicating room for optimization.\n",
    "- Skip-gram with Negative Sampling: Showed improved efficiency with a lower average training loss of 1.977957.\n",
    "- GloVe Scratch: Achieved the lowest training loss of 0.724803, indicating a more effective learning during training.\n",
    "\n",
    "#### Training Time\n",
    "\n",
    "| Model                          | Total Training Time |\n",
    "|--------------------------------|---------------------|\n",
    "| Skip-gram                      | 18m 4s            |\n",
    "| Skip-gram with Negative Sampling | 17m 8s             |\n",
    "| GloVe Scratch                    | 1m 54s              |\n",
    "\n",
    "\n",
    "- **Training Time:**\n",
    "- Skip-gram: Took 18 minutes and 4 seconds for training.\n",
    "- Skip-gram with Negative Sampling: Required 17 minutes and 8 seconds, slightly faster than Skip-gram.\n",
    "- GloVe Scratch: Was significantly faster with a training time of 1 minute and 54 seconds.\n",
    "\n",
    "This comparison sheds light on the trade-offs between models in terms of learning efficiency and computational needs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analogy Data Load and Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The load_data function loads and parses datasets for word analogy tasks, crucial in assessing word embeddings. \n",
    "\n",
    "It extracts analogies from a specified file, categorizes them (e.g., 'capital-common-countries', 'gram7-past-tense'), and returns categorized lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Sections for different types of analogies\n",
    "    capital_common_countries = []\n",
    "    gram7_past_tense = []\n",
    "\n",
    "    # Current section\n",
    "    current_section = None\n",
    "\n",
    "    for line in lines[1:]:\n",
    "        # Identify the section\n",
    "        if ': capital-common-countries' in line:\n",
    "            current_section = capital_common_countries\n",
    "            continue\n",
    "        elif ': gram7-past-tense' in line:\n",
    "            current_section = gram7_past_tense\n",
    "            continue\n",
    "        elif ': gram8-plural' in line:\n",
    "            break\n",
    "\n",
    "        # Split the line into words and add to the current section\n",
    "        if current_section is not None:\n",
    "            words = line.strip().split()\n",
    "            if len(words) == 4:  # Ensure there are exactly 4 words\n",
    "                current_section.append(tuple(words))\n",
    "\n",
    "    return capital_common_countries, gram7_past_tense\n",
    "\n",
    "# Load the data\n",
    "file_path = 'analogy-dataset/word-test.v1.txt'\n",
    "capital_common_countries, gram7_past_tense = load_data(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('Athens', 'Greece', 'Baghdad', 'Iraq'),\n",
       " ('Ukraine', 'Ukrainian', 'Switzerland', 'Swiss'))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the split is working okay or not for capital and countries\n",
    "capital_common_countries[0], capital_common_countries[len(capital_common_countries)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('dancing', 'danced', 'decreasing', 'decreased'),\n",
       " ('writing', 'wrote', 'walking', 'walked'))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the split is working okay or not for past tense\n",
    "gram7_past_tense[0], gram7_past_tense[len(gram7_past_tense)-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Syntactice & Semantic Accuracy Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The  `calculate_accuracy` function gauges the accuracy of either a skimgram or skimgram-neg-sampling model on word analogies, comparing predicted and actual embeddings. It serves as a metric for the model's capability in semantic and syntactic understanding of word representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, dataset, word2index, index2word):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for word1, word1_target, word2, word2_target in dataset:\n",
    "        # Check if all words are in the model's vocabulary, skip the analogy if any word is OOV\n",
    "        if all(word in word2index for word in [word1, word1_target, word2, word2_target]):\n",
    "            total += 1\n",
    "\n",
    "            # Get the indices for each word\n",
    "            word1_idx = word2index[word1]\n",
    "            word1_target_idx = word2index[word1_target]\n",
    "            word2_idx = word2index[word2]\n",
    "\n",
    "            # Get the embeddings for each word using the formula (center_embedding + outside_embedding) / 2\n",
    "            word1_emb = (model.embedding_center(torch.tensor([word1_idx], dtype=torch.long)).squeeze(0) +\n",
    "                         model.embedding_outside(torch.tensor([word1_idx], dtype=torch.long)).squeeze(0)) / 2\n",
    "            word1_target_emb = (model.embedding_center(torch.tensor([word1_target_idx], dtype=torch.long)).squeeze(0) +\n",
    "                                model.embedding_outside(torch.tensor([word1_target_idx], dtype=torch.long)).squeeze(0)) / 2\n",
    "            word2_emb = (model.embedding_center(torch.tensor([word2_idx], dtype=torch.long)).squeeze(0) +\n",
    "                         model.embedding_outside(torch.tensor([word2_idx], dtype=torch.long)).squeeze(0)) / 2\n",
    "\n",
    "            # Compute the expected embedding for the target word\n",
    "            expected_emb = word2_emb - word1_emb + word1_target_emb\n",
    "\n",
    "            # Calculate similarities between the expected embedding and all word embeddings in the vocabulary\n",
    "            similarities = F.cosine_similarity( (model.embedding_center.weight + model.embedding_outside.weight) / 2, expected_emb.unsqueeze(0), dim=1)\n",
    "\n",
    "            # Find the index of the maximum similarity (excluding the original words in the analogy)\n",
    "            indices_to_exclude = [word2index[word] for word in [word1, word1_target, word2] if word in word2index]\n",
    "            for idx in indices_to_exclude:\n",
    "                similarities[idx] = -1\n",
    "\n",
    "            max_similarity_idx = torch.argmax(similarities).item()\n",
    "\n",
    "            # print(index2word[str(max_similarity_idx)], word2_target)\n",
    "\n",
    "\n",
    "\n",
    "            # Check if the word with the maximum similarity is the target word\n",
    "            if index2word[str(max_similarity_idx)] == word2_target:\n",
    "                correct += 1\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The  `calculate_accuracy_GloVe` function gauges the accuracy of a GloVe Scratch model on word analogies, comparing predicted and actual embeddings. \n",
    "It serves as a metric for the model's capability in semantic and syntactic understanding of word representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy_GloVe(model, dataset, word2index, index2word):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for word1, word1_target, word2, word2_target in dataset:\n",
    "        # Check if all words are in the model's vocabulary, skip the analogy if any word is OOV\n",
    "        if all(word in word2index for word in [word1, word1_target, word2, word2_target]):\n",
    "            total += 1\n",
    "\n",
    "            # Get the indices for each word\n",
    "            word1_idx = word2index[word1]\n",
    "            word1_target_idx = word2index[word1_target]\n",
    "            word2_idx = word2index[word2]\n",
    "\n",
    "            # Get the embeddings for each word\n",
    "            # For GloVe, consider using the sum of center and outside embeddings for each word\n",
    "            word1_emb = (model.center_embedding(torch.tensor([word1_idx], dtype=torch.long)).squeeze(0) + \\\n",
    "                        model.outside_embedding(torch.tensor([word1_idx], dtype=torch.long)).squeeze(0))/2\n",
    "            \n",
    "            word1_target_emb = (model.center_embedding(torch.tensor([word1_target_idx], dtype=torch.long)).squeeze(0) + \\\n",
    "                               model.outside_embedding(torch.tensor([word1_target_idx], dtype=torch.long)).squeeze(0))/2\n",
    "            \n",
    "            word2_emb = (model.center_embedding(torch.tensor([word2_idx], dtype=torch.long)).squeeze(0) + \\\n",
    "                        model.outside_embedding(torch.tensor([word2_idx], dtype=torch.long)).squeeze(0))/2\n",
    "\n",
    "            # Compute the expected embedding for the target word\n",
    "            expected_emb = word2_emb - word1_emb + word1_target_emb\n",
    "\n",
    "            # Calculate similarities between the expected embedding and all word embeddings in the vocabulary\n",
    "            # Consider using the sum of center and outside embeddings for the similarity calculation\n",
    "            all_embeddings = model.center_embedding.weight + model.outside_embedding.weight\n",
    "            similarities = F.cosine_similarity(all_embeddings/2, expected_emb.unsqueeze(0), dim=1)\n",
    "\n",
    "            # Find the index of the maximum similarity (excluding the original words in the analogy)\n",
    "            indices_to_exclude = [word2index[word] for word in [word1, word1_target, word2] if word in word2index]\n",
    "            for idx in indices_to_exclude:\n",
    "                similarities[idx] = -1\n",
    "\n",
    "            max_similarity_idx = torch.argmax(similarities).item()\n",
    "            \n",
    "            # print(index2word[str(max_similarity_idx)], word2_target)\n",
    "\n",
    "            # Check if the word with the maximum similarity is the target word\n",
    "            if index2word[str(max_similarity_idx)] == word2_target:\n",
    "                correct += 1\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function, `calculate_accuracy_GloVe_gensim`, evaluates the accuracy of a GloVe model (loaded via Gensim) on a word analogy dataset. The function operates by leveraging Gensim's data structures and functionalities to calculate the expected embedding for the target word in each analogy and compares it with the actual embeddings in the model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy_GloVe_gensim(model, dataset):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for word1, word1_target, word2, word2_target in dataset:\n",
    "        # Check if all words are in the model's vocabulary, skip the analogy if any word is OOV\n",
    "        if all(word in model.key_to_index for word in [word1, word1_target, word2, word2_target]):\n",
    "            total += 1\n",
    "\n",
    "            # Get the embeddings for each word\n",
    "            word1_emb = model[word1]\n",
    "            word1_target_emb = model[word1_target]\n",
    "            word2_emb = model[word2]\n",
    "\n",
    "            # Compute the expected embedding for the target word\n",
    "            expected_emb = word2_emb - word1_emb + word1_target_emb\n",
    "\n",
    "            # Calculate similarities between the expected embedding and all word embeddings in the vocabulary\n",
    "            all_embeddings = model.vectors\n",
    "            similarities = np.dot(all_embeddings, expected_emb) / (np.linalg.norm(all_embeddings, axis=1) * np.linalg.norm(expected_emb))\n",
    "\n",
    "            # Exclude original words from consideration\n",
    "            for word in [word1, word1_target, word2]:\n",
    "                if word in model.key_to_index:\n",
    "                    similarities[model.key_to_index[word]] = -1\n",
    "\n",
    "            max_similarity_idx = np.argmax(similarities)\n",
    "\n",
    "            # Check if the word with the maximum similarity is the target word\n",
    "            if model.index_to_key[max_similarity_idx] == word2_target:\n",
    "                correct += 1\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Skipgram word2vec's Accuracy Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This section defines the `Skipgram` class, a neural network model for learning word embeddings via the Skipgram architecture. \n",
    "\n",
    "- After defining the model, the code proceeds to load a already-trained model along with its configuration and the word-to-index and index-to-word mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Skipgram model class\n",
    "class Skipgram(nn.Module):\n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(Skipgram, self).__init__()\n",
    "        # Embedding layers for center and outside words\n",
    "        self.embedding_center = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
    "    \n",
    "    def forward(self, center, outside, all_vocabs):\n",
    "        # Obtain embeddings for center, outside, and all vocabulary words\n",
    "        center_embedding = self.embedding_center(center)\n",
    "        outside_embedding = self.embedding_outside(outside)\n",
    "        all_vocabs_embedding = self.embedding_outside(all_vocabs)\n",
    "        \n",
    "        # Calculate top and lower terms for loss computation\n",
    "        top_term = torch.exp(outside_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2))\n",
    "        lower_term = all_vocabs_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2)\n",
    "        lower_term_sum = torch.sum(torch.exp(lower_term), 1)\n",
    "        \n",
    "        # Calculate and return loss\n",
    "        loss = -torch.mean(torch.log(top_term / lower_term_sum))\n",
    "        return loss\n",
    "\n",
    "word2index_path = './config_model_files/word2index.json'  \n",
    "index2word_path = './config_model_files/index2word.json' \n",
    "model_path = './config_model_files/word2vec_model.pth'\n",
    "config_path = './config_model_files/word2vec_config.json'        \n",
    "# Load model configuration\n",
    "with open(config_path, 'r') as config_file:\n",
    "    config_skipgram = json.load(config_file)\n",
    "\n",
    "# Initialize model with loaded configuration\n",
    "loaded_model_Skipgram = Skipgram(voc_size=config_skipgram['voc_size'], emb_size=config_skipgram['emb_size'])\n",
    "\n",
    "# Load model state\n",
    "loaded_model_Skipgram.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "loaded_model_Skipgram.eval()  # Set model to evaluation mode\n",
    "\n",
    "with open(word2index_path, 'r') as file:\n",
    "    word2index_skipgram = json.load(file)\n",
    "\n",
    "with open(index2word_path, 'r') as file:\n",
    "    index2word_skipgram = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipgram Semantic Accuracy: 0.00%\n",
      "Skipgram Syntactic Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the semantic accuracy on the 'capital-common-countries' analogy dataset\n",
    "semantic_accuracy_Skipgram = calculate_accuracy(loaded_model_Skipgram, capital_common_countries, word2index_skipgram, index2word_skipgram)\n",
    "\n",
    "# Calculate the syntactic accuracy on the 'gram7-past-tense' analogy dataset\n",
    "syntactic_accuracy_Skipgram = calculate_accuracy(loaded_model_Skipgram, gram7_past_tense, word2index_skipgram, index2word_skipgram)\n",
    "\n",
    "# Print the semantic accuracy as a percentage\n",
    "print(f\"Skipgram Semantic Accuracy: {semantic_accuracy_Skipgram * 100:.2f}%\")\n",
    "\n",
    "# Print the syntactic accuracy as a percentage\n",
    "print(f\"Skipgram Syntactic Accuracy: {syntactic_accuracy_Skipgram * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. SkipgramNeg Sampling word2vec's Accuracy Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this section, we define the `SkipgramNeg` class, an implementation of the Skipgram model enhanced with Negative Sampling for more efficient training. After defining the model, the code proceeds to load a already-trained model, including its configuration and the word-to-index mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "class SkipgramNeg(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(SkipgramNeg, self).__init__()\n",
    "        self.embedding_center  = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
    "        self.logsigmoid        = nn.LogSigmoid()\n",
    "    \n",
    "    def forward(self, center, outside, negative):\n",
    "        #center, outside:  (bs, 1)\n",
    "        #negative       :  (bs, k)\n",
    "        \n",
    "        center_embed   = self.embedding_center(center) #(bs, 1, emb_size)\n",
    "        outside_embed  = self.embedding_outside(outside) #(bs, 1, emb_size)\n",
    "        negative_embed = self.embedding_outside(negative) #(bs, k, emb_size)\n",
    "        \n",
    "        uovc           = outside_embed.bmm(center_embed.transpose(1, 2)).squeeze(2) #(bs, 1)\n",
    "        ukvc           = -negative_embed.bmm(center_embed.transpose(1, 2)).squeeze(2) #(bs, k)\n",
    "        ukvc_sum       = torch.sum(ukvc, 1).reshape(-1, 1) #(bs, 1)\n",
    "        \n",
    "        loss           = self.logsigmoid(uovc) + self.logsigmoid(ukvc_sum)\n",
    "        \n",
    "        return -torch.mean(loss)\n",
    "\n",
    "\n",
    "word2index_path = './config_model_files/word2index_neg_sam.json'  \n",
    "index2word_path = './config_model_files/index2word_neg_sam.json' \n",
    "model_path = './config_model_files/word2vec_model_neg_sam.pth'\n",
    "config_path = './config_model_files/word2vec_config_neg_sam.json'\n",
    "\n",
    "with open(word2index_path, 'r') as file:\n",
    "    word2index_SkipgramNeg = json.load(file)  # Load the word2index dictionary from the JSON file\n",
    "\n",
    "with open(index2word_path, 'r') as file:\n",
    "    index2word_SkipgramNeg = json.load(file)\n",
    "\n",
    "# Load the model's configuration from a JSON file\n",
    "with open(config_path, 'r') as config_file:\n",
    "    config_SkipgramNeg = json.load(config_file)\n",
    "\n",
    "# Retrieve the configuration values\n",
    "voc_size = config_SkipgramNeg['voc_size']  # Vocabulary size\n",
    "emb_size = config_SkipgramNeg['emb_size']  # Embedding size\n",
    "\n",
    "# Initialize a new Word2Vec model with the loaded configuration\n",
    "loaded_model_SkipgramNeg = SkipgramNeg(voc_size, emb_size)\n",
    "\n",
    "# Load the state dictionary (model parameters) into the initialized model\n",
    "loaded_model_SkipgramNeg.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# Set the model to evaluation mode (useful for inference)\n",
    "loaded_model_SkipgramNeg.eval()\n",
    "\n",
    "# Confirm successful model loading\n",
    "print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipgram-neg Semantic Accuracy: 0.00%\n",
      "Skipgram-neg Syntactic Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Calculate semantic accuracy on 'capital-common-countries' analogies\n",
    "semantic_accuracy_Skipgram_neg = calculate_accuracy(loaded_model_SkipgramNeg, capital_common_countries, word2index_SkipgramNeg, index2word_SkipgramNeg)\n",
    "# Calculate syntactic accuracy on 'gram7-past-tense' analogies\n",
    "syntactic_accuracy_Skipgram_neg = calculate_accuracy(loaded_model_SkipgramNeg, gram7_past_tense, word2index_SkipgramNeg, index2word_SkipgramNeg)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Skipgram-neg Semantic Accuracy: {semantic_accuracy_Skipgram_neg * 100:.2f}%\")\n",
    "print(f\"Skipgram-neg Syntactic Accuracy: {syntactic_accuracy_Skipgram_neg * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.  GloVe Scratch word2vec's Accuracy Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This section defines the `Glove` class, which implements the GloVe model for learning word embeddings by capturing global word co-occurrence statistics.\n",
    "\n",
    "- After defining the model, the code proceeds to load a already-trained GloVe model, including its configuration, word-to-index, and index-to-word mappings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "class Glove(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(Glove, self).__init__()\n",
    "        # Embeddings for center words\n",
    "        self.center_embedding  = nn.Embedding(voc_size, emb_size)\n",
    "        # Embeddings for context (outside) words\n",
    "        self.outside_embedding = nn.Embedding(voc_size, emb_size)\n",
    "        \n",
    "        # Bias terms for center words\n",
    "        self.center_bias       = nn.Embedding(voc_size, 1) \n",
    "        # Bias terms for context (outside) words\n",
    "        self.outside_bias      = nn.Embedding(voc_size, 1)\n",
    "    \n",
    "    def forward(self, center, outside, coocs, weighting):\n",
    "        # Retrieve the embeddings for the center words\n",
    "        center_embeds  = self.center_embedding(center)  # (batch_size, 1, emb_size)\n",
    "        # Retrieve the embeddings for the outside words\n",
    "        outside_embeds = self.outside_embedding(outside)  # (batch_size, 1, emb_size)\n",
    "        \n",
    "        # Retrieve and squeeze the bias for the center words\n",
    "        center_bias    = self.center_bias(center).squeeze(1)\n",
    "        # Retrieve and squeeze the bias for the outside words\n",
    "        target_bias    = self.outside_bias(outside).squeeze(1)\n",
    "        \n",
    "        # Compute the dot product of center and outside word embeddings\n",
    "        inner_product  = outside_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        \n",
    "        # Compute the GloVe loss as the weighted squared error between\n",
    "        # the log co-occurrence counts and the model predictions (dot product + biases)\n",
    "        loss = weighting * torch.pow(inner_product + center_bias + target_bias - coocs, 2)\n",
    "        \n",
    "        # Return the sum of the losses for the batch\n",
    "        return torch.sum(loss)\n",
    "    \n",
    "word2index_path = './config_model_files/word2index_GloVe_Scratch.json'  \n",
    "index2word_path = './config_model_files/index2word_GloVe_Scratch.json' \n",
    "model_path = './config_model_files/word2vec_model_GloVe_Scratch.pth'\n",
    "config_path = './config_model_files/word2vec_config_GloVe_Scratch.json'\n",
    "with open(word2index_path, 'r') as file:\n",
    "    word2index_Glove = json.load(file)  # Load the word2index dictionary from the JSON file\n",
    "\n",
    "with open(index2word_path, 'r') as file:\n",
    "    index2word_Glove = json.load(file)  # Load the index2word dictionary from the JSON file\n",
    "# Load the model's configuration from a JSON file\n",
    "with open(config_path, 'r') as config_file:\n",
    "    config_Glove = json.load(config_file)\n",
    "\n",
    "# Retrieve the configuration values\n",
    "voc_size = config_Glove['voc_size']  # Vocabulary size\n",
    "emb_size = config_Glove['emb_size']  # Embedding size\n",
    "\n",
    "# Initialize a new Word2Vec model with the loaded configuration\n",
    "loaded_model_Glove = Glove(voc_size, emb_size)\n",
    "\n",
    "# Load the state dictionary (model parameters) into the initialized model\n",
    "loaded_model_Glove.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# Set the model to evaluation mode (useful for inference)\n",
    "loaded_model_Glove.eval()\n",
    "\n",
    "# Confirm successful model loading\n",
    "print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe Semantic Accuracy: 0.00%\n",
      "GloVe Syntactic Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "semantic_accuracy_GloVe = calculate_accuracy_GloVe(loaded_model_Glove, capital_common_countries, word2index_Glove, index2word_Glove)\n",
    "syntactic_accuracy_GloVe = calculate_accuracy_GloVe(loaded_model_Glove, gram7_past_tense, word2index_Glove, index2word_Glove)\n",
    "print(f\"GloVe Semantic Accuracy: {semantic_accuracy_GloVe * 100:.2f}%\")\n",
    "print(f\"GloVe Syntactic Accuracy: {syntactic_accuracy_GloVe * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. GloVe with Gensim pre-trained model's Accuracy Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, Gensim is used to convert GloVe word embeddings to the Word2Vec format, facilitating compatibility with NLP tools that natively support Word2Vec.\n",
    "\n",
    "The `glove2word2vec` script facilitates this conversion, making GloVe embeddings readily usable in a broader array of applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l8/jmkvjw954ns16d74w484fdrm0000gn/T/ipykernel_1877/999121853.py:9: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(glove_file_path, word2vec_output_file)\n"
     ]
    }
   ],
   "source": [
    "# Path to the GloVe file (Replace this with GloVe file path)\n",
    "glove_file_path = 'glove-dataset/glove.6B.100d.txt'   #  download from https://github.com/allenai/spv2/blob/master/model/glove.6B.100d.txt.gz\n",
    "\n",
    "# File path for the output Word2Vec format file\n",
    "word2vec_output_file = glove_file_path + '.word2vec'\n",
    "\n",
    "# Convert the GloVe file format to the Word2Vec file format\n",
    "# This creates a new file in Word2Vec format at the specified path\n",
    "glove2word2vec(glove_file_path, word2vec_output_file)\n",
    "\n",
    "# Load the model from the converted Word2Vec format file\n",
    "# The binary flag is set to False because the Word2Vec format is text-based, not binary\n",
    "loaded_model_Glove_Gen = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe gensim Semantic Accuracy: 54.97%\n",
      "GloVe gensim Syntactic Accuracy: 53.40%\n"
     ]
    }
   ],
   "source": [
    "# Calculate semantic accuracy on the 'capital-common-countries' dataset\n",
    "semantic_accuracy_GloVe_Gen = calculate_accuracy_GloVe_gensim(loaded_model_Glove_Gen, capital_common_countries)\n",
    "# Calculate syntactic accuracy on the 'gram7-past-tense' dataset\n",
    "syntactic_accuracy_GloVe_Gen = calculate_accuracy_GloVe_gensim(loaded_model_Glove_Gen, gram7_past_tense)\n",
    "\n",
    "# Print the semantic and syntactic accuracies\n",
    "print(f\"GloVe gensim Semantic Accuracy: {semantic_accuracy_GloVe_Gen * 100:.2f}%\")\n",
    "print(f\"GloVe gensim Syntactic Accuracy: {syntactic_accuracy_GloVe_Gen * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model's Comparison Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model                             | Window Size | Training Loss(taken from Traning Notebooks) | Syntactic Accuracy | Semantic Accuracy |\n",
    "|-----------------------------------|-------------|---------------|--------------------|-------------------|\n",
    "| Skip-gram                         | 2           | 8.133966         | 0.00%              | 0.00%             |\n",
    "| Skip-gram with Negative Sampling  | 2           | 1.977957       | 0.00%              | 0.00%             |\n",
    "| GloVe (Custom)                    | 2           | 0.724803        | 0.00%              | 0.00%             |\n",
    "| GloVe (Pre-trained Gensim)        | N/A         | N/A           | 53.40%             | 54.97%            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of various models on the   `capital-common-countries (semantic)` and `past-tense (syntactic)` [analogy dataset](https://www.fit.vutbr.cz/~imikolov/rnnlm/word-test.v1.txt) reveals significant insights:\n",
    "\n",
    "- **Custom Models' Zero Accuracy:**\n",
    "Skip-gram, Skip-gram with Negative Sampling,and Scratch GloVe models demonstrated zero accuracy, likely due to the **out-of-vocabulary issue**.\n",
    "\n",
    "- **Strong Performance of Pre-trained GloVe:**\n",
    "Over 50% accuracy in both tasks, highlighting the importance of extensive training and diverse datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Find the correlation of WordSim353 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the correlation between our model's dot product and the provided similarity metrics, we'll use one of these files. Usually, the wordsim_relatedness_goldstandard.txt or wordsim_similarity_goldstandard.txt would contain the pairs of words along with human-assigned similarity scores.\n",
    "\n",
    "We'll go through the following steps:\n",
    "\n",
    "- Load the dataset.\n",
    "- Compute the cosine similarity for each word pair using our model.\n",
    "- Calculate Spearman's rank correlation between our model's similarities and the human-assigned scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Load the Dataset\n",
    "Let's load one of the gold standard files, parse it, and extract word pairs and human-assigned similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       word1      word2  human_score\n",
      "0   computer   keyboard         7.62\n",
      "1  Jerusalem     Israel         8.46\n",
      "2     planet     galaxy         8.11\n",
      "3     canyon  landscape         7.53\n",
      "4       OPEC    country         5.63\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the dataset file\n",
    "file_path = 'wordsim353_sim_rel/wordsim_relatedness_goldstandard.txt'\n",
    "\n",
    "# Load the dataset\n",
    "# Adjust the separator and column names based on the actual format of our file\n",
    "df = pd.read_csv(file_path, sep='\\t', names=['word1', 'word2', 'human_score'])\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Compute Model Similarities\n",
    "Now, let's compute the cosine similarity for each word pair using  model's embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function for skipgram and skipgram-neg-sampling model\n",
    "def compute_model_similarity(model, word_pairs, word2index):\n",
    "    model_similarities = []\n",
    "    for word1, word2 in word_pairs:\n",
    "        # Check if both words are in the vocabulary\n",
    "        if word1 in word2index and word2 in word2index:\n",
    "            word1_idx = word2index[word1]\n",
    "            word2_idx = word2index[word2]\n",
    "            \n",
    "            # Retrieve embeddings\n",
    "            word1_emb = (model.embedding_center(torch.tensor([word1_idx], dtype=torch.long)).squeeze(0) +\n",
    "                         model.embedding_outside(torch.tensor([word1_idx], dtype=torch.long)).squeeze(0))/2\n",
    "            word2_emb = (model.embedding_center(torch.tensor([word2_idx], dtype=torch.long)).squeeze(0) +\n",
    "                         model.embedding_outside(torch.tensor([word2_idx], dtype=torch.long)).squeeze(0))/2\n",
    "            \n",
    "            # Convert embeddings to numpy arrays after detaching them from the computation graph\n",
    "            word1_emb_np = word1_emb.detach().numpy()\n",
    "            word2_emb_np = word2_emb.detach().numpy()\n",
    "            \n",
    "            # Compute cosine similarity (use 1-cosine to convert distance to similarity)\n",
    "            similarity = 1 - cosine(word1_emb_np, word2_emb_np)\n",
    "            model_similarities.append(similarity)\n",
    "        else:\n",
    "            model_similarities.append(None)  # None if any or both words are OOV\n",
    "            \n",
    "    return model_similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_model_similarity_GloVe_Scratch(model, word_pairs, word2index):\n",
    "    model_similarities = []\n",
    "    for word1, word2 in word_pairs:\n",
    "        # Check if both words are in the vocabulary\n",
    "        if word1 in word2index and word2 in word2index:\n",
    "            word1_idx = word2index[word1]\n",
    "            word2_idx = word2index[word2]\n",
    "            \n",
    "            # Retrieve embeddings            \n",
    "            word1_emb = (model.center_embedding(torch.tensor([word1_idx], dtype=torch.long)).squeeze(0) + \\\n",
    "            model.outside_embedding(torch.tensor([word1_idx], dtype=torch.long)).squeeze(0))/2\n",
    "\n",
    "            word2_emb = (model.center_embedding(torch.tensor([word2_idx], dtype=torch.long)).squeeze(0) + \\\n",
    "                        model.outside_embedding(torch.tensor([word2_idx], dtype=torch.long)).squeeze(0))/2\n",
    "            \n",
    "            # Convert embeddings to numpy arrays after detaching them from the computation graph\n",
    "            word1_emb_np = word1_emb.detach().numpy()\n",
    "            word2_emb_np = word2_emb.detach().numpy()\n",
    "            \n",
    "            # Compute cosine similarity (use 1-cosine to convert distance to similarity)\n",
    "            similarity = 1 - cosine(word1_emb_np, word2_emb_np)\n",
    "            model_similarities.append(similarity)\n",
    "        else:\n",
    "            model_similarities.append(None)  # None if any or both words are OOV\n",
    "            \n",
    "    return model_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_glove_gensim_similarity(model, word_pairs):\n",
    "    model_similarities = []\n",
    "    for word1, word2 in word_pairs:\n",
    "        # Check if both words are in the vocabulary\n",
    "        if word1 in model.key_to_index and word2 in model.key_to_index:\n",
    "            # Compute cosine similarity using Gensim's method\n",
    "            similarity = model.similarity(word1, word2)\n",
    "            model_similarities.append(similarity)\n",
    "        else:\n",
    "            model_similarities.append(None)  # None if any or both words are OOV\n",
    "    return model_similarities\n",
    "\n",
    "def calculate_correlation(model, dataset):\n",
    "    # Assuming dataset is a DataFrame with 'word1', 'word2', 'human_score'\n",
    "    word_pairs = list(zip(dataset['word1'], dataset['word2']))\n",
    "    model_similarities = compute_glove_gensim_similarity(model, word_pairs)\n",
    "    \n",
    "    # Filter out pairs where at least one word was OOV\n",
    "    filtered_human_scores = [human_score for human_score, model_score in zip(dataset['human_score'], model_similarities) if model_score is not None]\n",
    "    filtered_model_scores = [model_score for model_score in model_similarities if model_score is not None]\n",
    "\n",
    "    # Calculate Spearman's rank correlation\n",
    "    correlation, _ = spearmanr(filtered_human_scores, filtered_model_scores)\n",
    "    return correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Calculate Spearman's Rank Correlation\n",
    "Finally, calculate Spearman's rank correlation between the model-computed similarities and the human-assigned scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman's rank correlation for Skipgram: 0.105\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# skipgram model, word2index loaded and the compute_model_similarity function defined\n",
    "model_similarities_skipgram = compute_model_similarity(loaded_model_Skipgram, list(zip(df['word1'], df['word2'])), word2index_skipgram)\n",
    "# Filter out pairs where at least one word was OOV\n",
    "filtered_human_scores = [human_score for human_score, model_score in zip(df['human_score'], model_similarities_skipgram) if model_score is not None]\n",
    "filtered_model_scores = [model_score for model_score in model_similarities_skipgram if model_score is not None]\n",
    "\n",
    "# Calculate Spearman's rank correlation\n",
    "correlation, _ = spearmanr(filtered_human_scores, filtered_model_scores)\n",
    "print(f\"Spearman's rank correlation for Skipgram: {correlation:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman's rank correlation for Skipgram-Neg-Sampling: -0.118\n"
     ]
    }
   ],
   "source": [
    "# skipgram neg-smapling model, word2index loaded and the compute_model_similarity function defined\n",
    "model_similarities_skipgram_neg = compute_model_similarity(loaded_model_SkipgramNeg, list(zip(df['word1'], df['word2'])), word2index_SkipgramNeg)\n",
    "# Filter out pairs where at least one word was OOV\n",
    "filtered_human_scores = [human_score for human_score, model_score in zip(df['human_score'], model_similarities_skipgram_neg) if model_score is not None]\n",
    "filtered_model_scores = [model_score for model_score in model_similarities_skipgram_neg if model_score is not None]\n",
    "\n",
    "# Calculate Spearman's rank correlation\n",
    "correlation, _ = spearmanr(filtered_human_scores, filtered_model_scores)\n",
    "print(f\"Spearman's rank correlation for Skipgram-Neg-Sampling: {correlation:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman's rank correlation for GloVe Scratch: -0.224\n"
     ]
    }
   ],
   "source": [
    "# GloVe Scratch model, word2index loaded and the compute_model_similarity function defined\n",
    "model_similarities_Glove_Scratch = compute_model_similarity_GloVe_Scratch(loaded_model_Glove, list(zip(df['word1'], df['word2'])), word2index_Glove)\n",
    "# Filter out pairs where at least one word was OOV\n",
    "filtered_human_scores = [human_score for human_score, model_score in zip(df['human_score'], model_similarities_Glove_Scratch) if model_score is not None]\n",
    "filtered_model_scores = [model_score for model_score in model_similarities_Glove_Scratch if model_score is not None]\n",
    "\n",
    "# Calculate Spearman's rank correlation\n",
    "correlation, _ = spearmanr(filtered_human_scores, filtered_model_scores)\n",
    "print(f\"Spearman's rank correlation for GloVe Scratch: {correlation:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman's rank correlation for GloVe Gensim: 0.491\n"
     ]
    }
   ],
   "source": [
    "correlation = calculate_correlation(loaded_model_Glove_Gen, df)\n",
    "print(f\"Spearman's rank correlation for GloVe Gensim: {correlation:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spearman's Rank Correlation Summary\n",
    "The Spearman's rank correlation analysis reveals varied performance among different word embedding models in aligning with human-perceived word similarities on [wordsim353_sim_rel data](http://alfonseca.org/eng/research/wordsim353.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model                     | Spearman's Rank Correlation |\n",
    "|---------------------------|-----------------------------|\n",
    "| Skipgram                  |  0.105                       |\n",
    "| Skipgram-Neg-Sampling     | -0.118                     |\n",
    "| GloVe Scratch             | -0.224                     |\n",
    "| GloVe Gensim              | 0.491                       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Skipgram: Achieved a correlation of 0.105, indicating a weak positive relationship between the model's similarity scores and human judgment.\n",
    "\n",
    "- Skipgram-Neg-Sampling: Registered a correlation of -0.118, suggesting a weak inverse relationship, indicating that the model's assessment of similarity tends to slightly oppose human judgment.\n",
    "\n",
    "- GloVe Scratch: Had a correlation of -0.224, demonstrating a somewhat stronger inverse relationship than the Skipgram-Neg-Sampling model, suggesting its assessments are more contrary to human judgment.\n",
    "\n",
    "- GloVe Gensim: Scored the highest with a correlation of 0.491, signifying a moderate positive correlation with human judgment, making it relatively the most aligned model with human perception among those evaluated.\n",
    "\n",
    "The pre-trained GloVe model from Gensim showcases superior performance, significantly aligning with human judgment in word similarities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Future research directions may include enhancing custom-trained models by:\n",
    "   - Expanding the training corpus to expose the model to a more varied text.\n",
    "\n",
    "   - Optimizing model architectures/ tuning the parameters to improve their learning capacity.\n",
    "Refining training processes to ensure more effective learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Computer-programming",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
